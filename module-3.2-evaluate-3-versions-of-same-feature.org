#+TITLE: Module 3.2
#+SUBTITLE: evaluate 3 versions of same feature
#+AUTHOR: Peter Jun Koh
#+EMAIL: gopeterjun@naver.com
#+DESCRIPTION: use AI to score code against a rubric
#+KEYWORDS: LLM, claude code, prompting, markdown, quality, rubric
#+LANGUAGE: en

* Summary

- Created on: [2025-07-26 Sat]
- Last Updated: [2025-07-26 Sat 19:08]

In Module 2.4 you may recall that we added a new feature to export expenses
to CSV. Each different version of the new feature was created in the
following 3 branches:

- ~feature-data-export-v1~
- ~feature-data-export-v2~
- ~feature-data-export-v3~

This is in addition to branch =main=.

The sections below will refer to these branches.

* Topics

** Prompt to evaluate 3 different branches of the same feature

#+begin_src markdown
  I have three different implementations of data export functionality across
  three git branches in my expense tracker application. I want to create a
  systematic evaluation framework to compare them thoroughly.

  BACKGROUND:
  - feature-data-export-v1: Simple CSV export (one-button approach)
  - feature-data-export-v2: Advanced export with multiple formats and
    filtering options
  - feature-data-export-v3: Cloud integration with sharing and collaboration
    features

  Now I want you to systematically analyze each of three features
  implementations by switching between branches and examining the code,
  architecture, and implementation details.

  ANALYSIS PROCESS:

  For each branch (v1, v2, v3), please:

  1. Switch to the branch
  2. Examine all the files that were created or modified
  3. Analyze the code architecture and patterns used
  4. Look at component structure and organization
  5. Review the user interface implementation
  6. Check for error handling and edge cases
  7. Assess the technical approach and libraries used

  DOCUMENTATION:

  Create a file called "code-analysis.md" with detailed findings for each
  version:

  ,**For Each Version, Document:**
  - Files created/modified (list them)
  - Code architecture overview (how is it organized?)
  - Key components and their responsibilities
  - Libraries and dependencies used
  - Implementation patterns and approaches
  - Code complexity assessment
  - Error handling approach
  - Security considerations
  - Performance implications
  - Extensibility and maintainability factors

  ,**Technical Deep Dive:**
  - How does the export functionality work technically?
  - What file generation approach is used?
  - How is user interaction handled?
  - What state management patterns are used?
  - How are edge cases handled?

  Be thorough and technical - this analysis will inform our decision about
  which approach to adopt or how to combine them.
#+end_src

** my-evaluation-template.md

You should create your own markdown evaluation template that you can use
when you have multiple implementations of a feature which you want to
compare. This is your /Personal Evaluation Framework/. Once you have this
you can instruct ~claude code~ to run this against multiple branches or git
worktrees.
